#Digital History Notes

##January 11, 2016

The problem with online databases. They are created and curated by humans, not objective. Problematic. 

Git and github. Git takes pictures of changes to file, github allows you to share with others

Searching online newspapers. We don’t have a theory of search. We haven’t allowed search engines to find synonyms etc. our search engines are subtly changing our research and results. People cite the online globe and mail, cite it as the paper copy, but don’t acknowledge the digital version. We should because OCR is really bad for newspapers, because OCR is designed for legal documents, with clear texts, with newspapers, will make errors. Keyword search misses up to 40%.  A lot of trouble when historians pretend to look at physical object, versus digital. 

White guys in southern California, making software that reflects to world view of white guys from southern California.
The DiRT directory for digital tools

Two options 1) teach you to do data-y stuff. Will work for a while, then everything will change, become obsolete 2) teach world how to do data-y stuff as we learn how to do it.  
Git.github.com/shawngraham

Each week, people will do best at following a tutorial, then you will teach it to us while pointing out the gotchas the things that don’t make sense, connect the tutorial to readings from this class, other classes, research etc. Tie it into our historical practice.
Set up a github account, don’t have to use real name if you choose not to. Can, can be useful in growing a professional identity online, but also don’t want to open yourself up. 

Send top three topic choices

Look for assumptions that the tool is making and how that will influence your research.

Git – software invented by the guy who invented linux, sits on computer, watching specific folders. Ask it take a snapshot of folder. Can share snapshots with others. Not necessarily intuitive, common sense. Hub is online place for putting snapshots up to share with others. 

PyKwiki uses markdown run through github, using command line on pc to install. Uses python programming language

Need a text editor. Let’s you work with any type of file. Either atom.io or sublimetext.com

To get started, need a text editor, a github, work through first 3 exercises from other class by next week.

Notational velocity, a series of infinite notecards. For mac. For windows resophnotes

$ means mac, not windows.

Avoid mkdocs on windows 10

Make notes on how you get to each point in case something breaks. 

##January 18

Keep notes in a open notebook keeping track of what you do. Add, commit, push to github. Add and commit two parts of same action. 

Step 1 and step 2.  Add to it, and then commit a snapshot. Keep in a repository, commit to desktop tracking folder

Final project? To reveal the mysteries of the DH topic of today’s conversation. Bundling up digital history skills that we learn to show and teach others. “expert systems” shall make a “clippy” for digital history. Google “expert systems” and “WTFCSV?”

Twine as an expert system. What do video games actually do? How to play-instructions, manipulates emotions..

Download hypothes.is to add annotations to tutorials (programing historian)

Use philome.la to publically share games with the world

Final project should explain to people how to do digital history, what is digital history

Next week do tutorial, tie it to our research, where are you coming from in terms of historical practice. 

itch.io allows you to monetize your twine
 
##January 25

Github twine-wrapper. Twine as a platform for interactive experiences. Designed for mac, not windows. Will take game created and turn it into a desktop app. Windows are on their own to figure it out. Also web2executable, does the same thing. At the end you have a formal program. Good for cv, more impressive than just making a website. 

github/tmberton research notebook- pandoc tutorial

Markdown tutorial: pretty straightforward. The handbook exercises had a very similar tutorial. No problems with it, showed examples. Laurel writes in html, not markdown. Felt that the transition was difficult. Html has a lot of non-texual stuff, whereas markdown is really stripped down, anything can read it. Easy to write and read. Can use editors on smartphone or tablets. Columns in markdown: google it. classeur.io looks like a notebook, can tie it github but can't find the files on own machines. Markdown allows you to focus on more than just the format. Typora.io merges preview with markdown, can get in the way. Markdown strips away all of the other stuff. Time to get crap done. Says there are only  four levels of headers, can go up to 6 levels of headers. 

Pandoc tutorial: Can use pandoc to go back from word documents to a plain text, markdown. Zotero has a web extension to click and get citation. Can keep reading notes in it, can share citations, can write notes in markdown in zotero. has a plug in for word. LaTeX, turns things into pdfs (already have). YAML block. Yet Another Markdown Language, tell it the important stuff. 
----
Title: 
Author:
Date:
----
**control-shift-t: restore webpages in browser**

Images: make sure it is in the same file as markdown file. ![img](img/image.jpg) have to end in jpg. 

Citations go to website to change citation styles. Pandoc doesnt make a separate page for bibliography. Getting them to work was the toughest part. Can use pandoc to change style. How? 

Open access research. SSHRC is requiring that research is available for several years. Publishing research notes, data sets, certain journals will publish these. 

The environment, what did you have stuff installed, what machine did you use. Really have to explain what is going on, what files/programs are involved in the process. Make what you do transparent, talk to the world. "Hacking the 'Papers of You'" See this a curating yourself and your work. 

Open notebooks, research notes for myself, write in a certain way because no one will read them. Would take notes in a very different way to write knowing others could read them. Research as performance would change it. Journal of Open Humanities Data. 
electricarchaelogy.ca

Methods of Digital Analysis: Examples from BC Stats: Martin Monkman. Provincial Statistician and Director, BC STats. bayesball.blogspot.ca. The pushback against the move forward of open notebooks, markdown etc can be found in all fields. In the golden age to take advantage of this new technology, markdown still being developed, not settled on and universalized yet. Good decisions start with good data. 

The BC Public Service Work Environment Survey. Biennial census survey of the entire BC Public Service. "What one thing would you like your organization to focus on to improve your work environment?"  In 2006-2011 this coding was done manually, a team of coders read each comment and assigned it to an appropriate code. Labour intensive, time consuming, costly. 2013 use of SPSS Text Analytics. Themes and subthemes. 

Edward R. Tufte: "Graphics are instruments for reasoning about quantitative information."

Different graphs, different tools tell different parts of the story (BC elections)

NY Public Library Digital images just released. With digital tools, there are a trail of records. BC email records. Set up a network diagram. The email logs contain and reveal personal information through the network diagram. 

Google maps, in the municipalities have had to build new road because Google sends people down a logging road that doesn't exist. It's easier for them to build a road than to change google

##February 1

Part of assessment is to be reflective in a public sphere. Either a wordpress blog or notes on github. In github account there is a special repository you can create. Call it your username.github.io everything in there, github will put in on interwebs as an actual website. Go to guthub jekyll now, fork it. Can just use a wordpress. What should the reflection cover? Be reflective. Think about experiences, feelings about it but then someone said something which ties into another class etc. Pymarc is a package, someone made it because people needed it. A lot of scholarly labour in that. Craft your notes into a narrative. 

hypothes.is a plugin that lives in chrome, allows you to annotate to the page, a public annotation. Can use with the programing historian. 

Wget. A free piece of software to download content from the web. Part of free software movement Came across the mission statement of wget. "to preserve, protect and promote the freedom to use, study, copy, modify, and redistribute computer software, and to defend the rights of Free Software users." Code as political philosophy. 

Problems with powershell, get GIT BASH. Equivalent to a mac terminal. Command prompt is in DOS, GIT BASH is in Unix. Emily also downloaded the zip file by mistake as well. Need to run as administrator, paternalism from the 1970s. Can set it up to get files of a particular type. Can't use on databases. Works on sensible, concise urls. Emily used it on LAC files, made a txt file to tell it what to do.

Entered (in the location of the text file):
wget -i name.txt

Save as text file (file name: e167.txt):
http://data2.archives.ca/e/e167/e004157817.jpg

http://data2.archives.ca/e/e167/e004157818.jpg

Insert command:
wget -i e167.txt

To include time limits, add at the end. Good internet citizens need to do this, otherwise can be black listed.

webarchives.ca 

Internet Archive. Sara didn't finish it. Programming Historian tutorial doesn't make it clear that it is formatted for windows. sudo = mac. How to solve problems. Tutorial assumed a lot of extra knowledge. Language/assumptions. who's in/who's out. Maybe have a glossary. Use hyphothes.is to suggest these things. Second version of programming historian, first was in a sequence. Some of the lessons are getting creaky, because they were written a while a go. Type python, get to interpreter, shown by >>> rather than >. everytime you go into the interpreter, it's just vanilla python, when you want to use extra packages, need to tell it to import (i.e. import internetarchive). Write a program intsead. In text editor. To run it, at the prompt, say "python file.py".  Indents matter. Use either tab bar or space bar, not both. 

Other databases may be more complicated. Getting the identifier, can just copy it into text file to use in wget. Rest of lesson is to get metadata marc. Can you open it through a certain pathway to have the files in a certain folder? marc.xml a pattern for describing metadata in a standard way across institutions. Carleton does not use marc.xml, metadata is useless, only has date digitized. 

pymarc lets you extract the exact info you want. Saves from reading each file individually. Not strictly necessary to actually be able to do this at this point in digital history. Makes more sense when you have a research question that you want to answer and want to use this program, skill to do this. 

Remember that the tutorial is assuming a computer that is already perfectly set up and configured in a particular way. Having issues and errors because we are pushing our computers and ourselves past what we know and what we do. SHowing your script allow people to see the strength of your data. This idea that your data may not be the best because your script missed some stuff. 

Read ryancordell.org "'Q i-jtb the Raven': Taking the Dirty OCR Seriously" Certain programs come out of a particular type of political milieu. Easy digitization means history influenced by metropolis focus (Globe and Mail and Toronto Star). Metadata tells you about what is embedded in pdf. 

To make a command code in markdown or on slack. use `write command and close it `. Use create a snippet on slack to write long code.

Stack overflow, community of help. Go for help with problems.

Anaconda: a development of python that comes with a whole bunch of extra programs. A someday, not today type of file. 

Before you do SPARQL, go to workbook.craftingdigitalhistory. Read exercises module 2, and How do we find data? blurb. Don't do the exercises themselves.   

##February 8

Digital History annotating the programming historians tutorials Feb 8-12 using hypothes.is. Already have an accumulated body of knowledge about the gotchas and whatnot as we have gone through the tutorials. #DHannotates. Amanda Visconti. literaturegeek.com

APIs and Zotero. Three tutorials. If you have it on the computer, could use your own personal library to find the metadata, the third one because Sara's own personal library didn't have any html files to use. Found using your own info not that useful, unless you have a lot of sources and you are interested in the metadata. 

Whole point of API is so you can interact with database without going through their website, can set up an automatic database to search. We are doing a lot of theoretical stuff, so the point of Zotero specifically may not be useful, but the skill of going into someone else's database and put a wrapper around it. API does the highlevel plumbing.  Can use it to keep track of what you have been finding at LAC. Digital Library of America have an API. 

Sara saw connections with open access, github for sources. Formats for you. Used stackoverflow to put screenshot pictures online in github. Could put them in same folder of github and make image name the link. Used the Zotero standalone, made an account online, synced account.  LOOK AT SARA's GITHUB. github.com/sarahollett/Reflections/blob/master/IntrototheZoteroAPI.md Something has been changed in Zotero that hasn't been updated yet, couldn't create a new item. 

In Digital history, either commit to keeping it updated for forever, or you have a statement that says "this worked in this context, as of this date." Need a sunset clause in work. Shouldn't feel bad when something doesn't work, learn more in that case, figured out an issue in their tutorial. 

Third tutorial, need to look over like ten different tutorials before actually doing that tutorial. Could ignore and not do. 

What assumptions are being made, both in digital history and in normal history. What assumptions do you have about your audience. Writing tutorials are extremely hard, not appreciated as being harder. 

Unicode issue. First tutorial, the error that all windows machine received. Found unicode command from stackoverflow and plugged it in. Idea of different fonts. Word square boxes instead of text. The font was not compatible with word. When on command line on windows, very restricted kind of fonts, Apple and Linus machines are using a very specific character language. Error messages are coming from an era when computers have less memory, very cryptic error messages. Always Google the error message. 

Most major libraries, archives, museums do have APIs, but not all websites have them. serendip-o-matic.com Most interesting about API is to build a larger tool. LAC does not have an API. SHould have data exposed with API as a good internet citizen, otherwise what is the point. opencontent.org to publish peer-reviewed data. 

SPARQL It was really good at explaining what it was, and what to do. Used visualization. What was SPARQL trying to do. URI's and literals it became confusing. URIs translate it into something that the webpage can read. URL takes you to a webpage, URI takes you to a concept. YAY terms to review were there. 

URIs and Literals. URIs are the links that connect things, whereas literals is the english language statement of the connection. A way of organizing data so people can access it. Not a lot of people are using it yet. With SPARQL you can link databases together to make it more streamlined. 

Problems. You have to know what you are looking for before you go looking for it. Point is looking at the connections between the things you want. Looking at context and relationships. Really wanted to use it as a language and a way of doing things. Talking about what you were doing, but allowed you to just click through it, Phoebe experimented with it. Took screenshot about useful prefixes. Struggled to figure out how to run her own queries. Go to website to look at specific informations, to know what to put after the prefix. In terms of search language, did they understand it? 

hypothes.is anybody who has already presented a tutorial, get a hypothe.is account, figure out how to use it. Go to your tutorial and mark it up to put in the gotchas and issues so others don't have to suffer. Can do it to ones that people struggled with, even if it wasn't your week. 

##February 22

Found a way to make a visualization of SPARQL automatically. In slack if interested. This week would have been quite useful to have done this week's lessons at the beginning of the semester.

Bash Command Line and Counting and Mining with Unix. learned a lot about git bash prior to this week. A lot was old news. What is the utility of the command line? Why would you use it? Elise talked about it in practice blog, Easy to combine files into a larger file. You can get more precision. Ian Milligan, seamless experience. The command line will immediately output what you're doing, including errors, good to track your error logs. What about reproducibility? The need to write down all the steps when helping a family member do something with computers. Command line advantage, the commands work regardless of edition of windows. Would be useful to look at preserving your research material, to learn to structure your files to be intuitive. Helps with future proofing your stuff and being able to find stuff again. So easy to be messy with computer folders. 

tagspaces.org a markdown editor but will also tag all of the files so you can find stuff again. Emily had to set up a tag system for parks canada nunavut.  

Stephen Ramsay, Life on the Command Line. 

How do you see the command line as intergral to other tutorials/work? Found the counting useful, gets rid of a lot of the labour that goes into history work. With command line can set up a workflow that works for you. Efficiency. Does digital history change your perspective in a specific and significant way, or is it just efficiency? Offers accountability? 

Voyant-tools.org way to visualize text data. You can have it downloaded to run on your machine so that you never lose your stuff. Can make a word cloud. Can ignore the simple words (the, and, of, to) by clicking on cogwheel. github/elisebigley has a tutorial from a different class for voyant. Super useful program to read, analyse the sources. 

Open Jesuit zip file. Unzip onto computer. Now have all of this data on my computer. 

Regular Expressions. Regular expressions are a sequence of characters that define a search pattern for use in find and replace operations. Different on whichever text editor you use. Figure out the syntax of the editor you are using. Be familiar with text. Got complicated really fast. Author didn't explain gotchas or the point of the tutorial. Didn't do a good job. RegExr was really useful, has a cheatsheet to help you figure out how to find stuff out. The tutorial in slack that Shawn wrote was easier to follow. REGEX and the Republic of Texas. 
[0-9] {4} to find strings of four numbers (dates)

Whatever tools that work best for you. REGEX is very much tailored to the specific project. Do things the ways that work the best for you. 

If you are on command line to break up a big file into little files, use "split -p 'pattern' file.txt" Tells computer to split the file using the pattern you're looking for using regular expressions.  E: [0-9]{4}, R: ~DATE/1 So you would split by date by saying "split -p '~DATE' file.txt"

For final project, should the Twines all be connected? Do them in a specific order to have them make the most sense. Or should they be separate. Think through how to make the project helpful through Twine, include the deadline, show the previous choices to explain why they reached a dead end and allow them to go back the necessary steps. Make it a story? Surprise and astound. Demonstrate that you have been awake and that these issues have made an impact. Could we maybe set it up so that a person could pick the type of project that they wanted to do, and follow the relevant tutorials with our help from a windows perspective? Connect the links so they can click and then follow along. Wants an unessay reflection. Can be an essay, a video, etc. Can collaborate if you want. 

February 29

Realized that the OCR lesson sucked. google ocr tesseract. Had a really good tutorial by Ben Schmidt for Mac users. Go to tesseract-ocr at github. Not mission critical. Is installed on computers in the resource room. Would use it through the command line. diybookscanner.org teach you how to build your own scanner (like physical scanner) with the needed software. Raspberry pi, a little tiny computer (30 buck) can even sign them out at the library. rdmilligan.wordpress.com ocr on raspberry pi. 

OpenRefine. Did it seem valuable? Last year, Laural worked on a project that involved scrapping a bunch of data. needed to see how many times certain names show up. Needed to clean up any alternate spellings, anything that didn't come out properly in the scrapping. OpenRefine really helped with that. Tutorial showed different ways to clean it up. Numeric stuff was new for her. Found GREL a little confusing but when it was broken down it makes sense. GREL is a specific language that Google uses in house. Clustering and collapsing of data, especially with messy scans is super helpful. 
	
	I found it really useful in that you could quickly get rid of different spellings or issues in the categories and keywords, and then see the categories and how many files fell under a specific category. 

OpenRefine scatterplots. Can make them with OpenRefine. Why not just you Excel? OpenRefine is designed to deal with a large amount of textual, numeric, mixed data. Does the spelling stuff for you. Excel can only really deal with numeric. You could cluster not just for spelling issue, can combine terms that made the sense as being the same in your own research. Still need to use your historical sense. Doesn't replace the imput data until you export it (and you can keep it that you can see original by changing the file name in the export version). 

Would need to regex your own text files to turn it into a table form before OpenRefining. 

Didn't really explain what each method specifically did in the clustering tool. They are various statistical tools for clustering things. Different algorithm that have different views of how the world works. 

Can use regex in OpenRefine, but it helps if they are at least in rows first. If you had a bunch of letters, if you had the metadata on the letters, might be more useful to use OpenRefine. If you have them one letter per row, you may be able to use it for text mining. What if you wanted to created the metadata? Would you use this? Or topic modelling? How do you create metadata for things? When they are scanned, is the metadata manually put it in? Usually. A regular expression might be the way to go to take the date info, for example, take it out and use ||| or something to turn it into a spreadsheet. 

Generating an Order Data Set from an OCR Text File. The tutorial was recommended for advance users. Basically, the tutorial is less lets do this, lets do that. It's trying to show the intersection between python and OCR data files. Way to take metadata and use it in a way that python can understand. Unless you get a job as a developer, this tutorial is kind of useless. Meant to give use Cleaning OCR text with Regular Expressions tutorial by O'Hara. http://programminghistorian.org/lessons/cleaning-ocrd-text-with-regular-expressions.html Works it out as a python script, instead of working out a regex each time. Good to build little lego blocks, instead of having to reinvent the wheel each time.  Don't use "sed" or "grep" really easy way to mess up 6 months worth of work. 

Let's do this tutorial together. 

Today if feels as though I have finally "drank the cool-aid" in terms of this class. I feel like I finally have enough basic information to not be horrified by the tutorials. 

To copy and paste a table out of a pdf and have it work as a table. Use the program Tabula, tabula.technology. 

If you are using Twine github.com/drumanagh/twine-wrapper will turn your twine into a desktop app. There will be a folder to put the twine game, type the command to turn it into an application, it will turn it into an app for a mac, a windows, a linux etc. Hasn't figured it out as a mobile app yet. Allows you can show a piece of software that you may. Handy in terms of archiving, websites die. Good in terms of issues with internet connectivity. Doesn't have to twine, any HTML, could use this with a website.  

Writing how to write OCR, would be a good thing for us to do. 

Phoebe found a tutorial for tesseract for macs. ryan.fb.github.io has lots of interesting code and whatnot on his site. Check it out. 

github.com/karpathy/char-rnn. Uses multi-layered recurrent neural Networks. What google uses for autocorrect. Can take a bunch of archived songs, and then have it engineer its own song. Can use it for text files, other types of files. THIS SEEMS INSANELY AWESOME REMEMBER THIS. 

sonification. programminghistorian.github.io/ph-submissions/lessons/sonification

March 7

Hollis, Topic Modelling and MALLET. "j.mp/hollis-march-7" Can be automated or done by hand. Words related to war, governance or both. Green = governance, yellow = war, red = both. Topic Modelling helps you find different topics in a number of documents that you may not have seen. Helps to decipher themes. MALLET can automatically analyse these topics. Sara M found it the best written tutorial thus far. See on slack, you can use MALLET directly in the command line or use a GUI. Hollis used the GUI version. Used this program to analyse his thesis. Came up with topics of stopwords, had to figure out how to get rid of them. Stopwords will be a place where your historical skills will come into play. Sometimes pronouns are important (ex. He = God).

www.ideals.illinois.edy/handle/2142/45709 Ted Underwood.  List of stopwords used in topic modelling journals, summer 2013. Needed to tell us what stopwords were removed to show what might be missing. When making your own stopword list, one word at a time. 

One of the outputs of the topic modelling tool does is arrange them as a website so you can click through to see which documents are big with each document. The Java GUI is good for a really quick topic model. Not much customizing though. Command line allows more fiddling of parameters. 

Just deleted something, hopefully not important. Oh, topic modelling is used without criticisim, has been abused. Topic modelling is a simulation. The biggest issue is that it is impossible to know when you start how many topics you should have. Too few and you lose nuance. 8-bit philosphy. 

Overview, not topic modelling but respects the original order of words. overviewdocs.com. Runs similarly to OpenRefine.

Remember that digital history is inextricably linked to asethetic and performance. Voyant is about counting number of words and visualizing their frequency, overview counts words and looks at distribution. Topic Models finds semantic patterns. 

Ben Schmidt vector models, uses R, difficult. Used it to find genered language patterns in ratemyprof. 

Rawdensitydesign.com shows flows of conversation as they change over time (Hollis' project) a different way to visualize. 

AntConc comparing corpora the most useful aspect. Built in from the beginning, easy to use. Better than topic modelling or voyant.

workbook.craftingdigitalhistory.ca > Communicating your Findings. A helpful source of resources about visualizations as well as exercises on visualizations. Think about typography and fonts in your Twine. Don't use comic sans. Think about colour. Assessibility and design. Should we have images? Leaflet powered maps. 

For Twine, look up Dan Cox twine tutorial.  Check out Melissa Ford's website has stuff about interactive writing on twine. 

March 14

New Cloud Atlas. A global effort to map each data place that makes up thecloud in an open and accountable way. We have set out to find and map each warehouse data centre, each internet exchange, each connecting cable and switch. Built on top of the open street map platform. 

Georectify. Got a map. Because all maps are a distortion of the world surface, to use a historical map we have to tell the computer where the points of the globe are so that it distorts it properly. Do so by pinning it using websites like (warp.worldmap.harvard.edu). Used Champlain's map of Ottawa and fire insurance map of Ottawa. 

On the warp.worldmap.harvard.edu, make an account. upload image of the map. Make up a name for the file that hasn't already been chosen. Click rectify. Gives two images, the historical map and the open street map. Start adding control points on each side. At least three points. Than click warp image.

Fork xtina-r/daea. Go to index, go to script and click edit:

var map = L.map('map', {
    center: [47.5668, -52.7120],
    zoom: 13,
    layers:[
      L.tileLayer('https://stamen-tiles-{s}.a.ssl.fastly.net/toner/{z}/{x}/{y}.png'), [This line gives the base map]
      L.tileLayer('http://warp.worldmap.harvard.edu/maps/tile/4322/{z}/{x}/{y}.png') [This gives the rectified]
      
the url from warp doesn't have {}brackets, change that. With some fusting around you can layer multiple maps. Make changes and commit changes. Need to change the coordinates (for Ottawa - center: [45.4, -75.7]. Commit changes. May have to change zoom settings as well. 

maps.stamen.com

Go to script:

 //enter path for .csv here
            omnivore.csv('sites/aa-sites-popup.csv')
             .on('ready', function(layer) {
             
Go to site directory in github/daea. Define the points centreblock coordinates, victoria coordinates, corner of sparks and elgin coordinates. the csv file has coordinates, historical time period, ignored time period. Uses a bit of html to style the names of location. Quotations for site info to prevent the comma from messing things up.  To get coordinates go to gps visualizer.

Go into sites, change a site to p.html for parliament.

Centre Block Parliament: 45.42506, -75.699829 (45.42506, -75.699829, Centre Block Parliament, "this is a point",<a href="sites/p.html">

Palladio. Possible projects? How did you guys find it? Can be a little difficult if your excel spreadsheet isn't uniform. Lends itself to certain types of spreadsheets, a comprehensive data set, helps if it lends to maps and if it is visual. Another option for spreadsheet data is raw.densitydesign.org.

We don't do a good job of teaching the consumability of maps. The visual is given deference, seen as more true almost, having more weight. 

Need to think about who your public is? Are you using this for your own understanding or presentation? 

Timeline JS a timeline builder. Can embed it into webpage. Storymap JS, allows you to imbed material into a leaflet map. 

March 21

Network analysis and network visualization. social network analysis and social network visualization are not the same thing. Analysis can be done without ever drawing a visualization or network. Dots = nodes. Need to be the same type of thing. Can easily connect them for a visualization, but a metric would not be valid if say some were teacher and some were students, bi-partite network. Links between them= edges. Can start making assumptions about the relations when you start mapping it out. Called a triad when the three nodes are connected in a triangle. Edges can be directed or undirected. Undirected makes no assumptions about analysis between relationships. For an information flow, directed edges makes a difference. Usually will be undirected, unless you know for sure (letters for example). For social network analysis, the type of node, make sure they are all the same type. Edges, can be a person itself, weight of the link is number of women for example. Edges can have weight. 

What can you get from visualizatio? many historians don't use analysis, because it is difficult, whereas visualization is easier. If you can discern a relationship, anything can be a visualization. Palladio doesn't allow for analysis, only visualizes in a single way. Could have gone into more detail in his coding process, why did he choose the seven that he did? Describe his tables a little more. How did you get from the archive, to how your data is currently set up? Often people does network analysis because that is what they know how to use. How much the method obscures as they reveal. 

Elise liked that During was transparent about what we as researchers contribute to the coding schemes. Data coding and visualizations vs. question-driven close reading and interpretation are not separate processes but are intertwined. The myth of objectivity. 

Make sure you understand the baisc rationale behind any centrality and layout algorithms, as they will affect your view on your data. How you think about your data and their relationships affects the results. 

Scott Weingart: Generally network studies ares made under the assumption that neither the stuff not the relationships are the whole story on their own. If you're studying something with networks, odd are you're doing so because you think the objects of your studyare interdependent rather than independent. Rerpresenting information as a network implicitly suggests not only that connections matter, but that they are required to understand whatever's going on.

Metrics. One measurement of centrality= number of connections (closeness centrality). weak ties are the glue that hold together social activites, those who connect separate cliques. Helps people get jobs etc. metric of betweeness centrality. The people on whom some phenomenon turns. How you calculate someone who is betweeen, look at each pair of individuals in turn, count the links, count number of shortests, the person who sits upon the most shortests paths among all of the pairs, that person is most between. 

Can use social networks for dissemination of ideas. Theories of evolving networks, how do networks actually form. NetLogo.

gephi.org. tool for social network analysis. Never save in .gephi format, save in .net or .html. Easy way to get data into gephi, is to use a text editor and make an adjacency list. using colons. gephi.org/users/supported-graph-formats/csv-format/ Save as a .csv will tell gephi it is undirected.

